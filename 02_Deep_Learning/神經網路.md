## 神經網路是什麼？

神經網路模仿人腦神經元結構，用來處理複雜的非線性資料模式，是深度學習的基礎。

- 不像傳統機器學習靠人定義規則，神經網路能自動學習高階特徵
- 應用場景包括：圖像辨識、語音辨識、自然語言處理、推薦系統等

---

## 基本結構與流程

神經網路包含三大部分：

- **Input Layer**：輸入特徵
- **Hidden Layers**：1 或多層神經元，進行加權、非線性轉換
- **Output Layer**：輸出預測結果（分類 / 回歸）

![神經網路示意圖](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/1920px-Artificial_neural_network.svg.png)

---

## 神經元怎麼運作？

每個神經元會做這些事：

1. 接收輸入：`x1, x2, ..., xn`
2. 計算加權和：`z = w1*x1 + w2*x2 + ... + wn*xn + b`
3. 通過激活函數轉換：`y = activation(z)`

### 常見激活函數：

| 函數 | 圖示 | 特點 |
|------|------|------|
| Sigmoid | ![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg) | 壓縮到 0~1，常用於輸出機率 |
| Tanh | ![tanh](https://upload.wikimedia.org/wikipedia/commons/c/cb/Activation_tanh.svg) | -1~1，對稱、梯度大於 sigmoid |
| ReLU | ![relu](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg) | 大於0傳遞，小於0設為0，簡單有效 |

---

## 損失函數與學習流程

學習的目的是讓預測接近真實值。這過程稱為「模型訓練」。

1. **Forward**：資料進入神經網路 → 得到預測結果
2. **Loss 計算**：與真實值相比，產生誤差
3. **Backward**：反向傳播（Backpropagation），計算每個參數對誤差的影響
4. **更新權重**：用梯度下降（Gradient Descent）調整參數

常見損失函數：

| 問題類型 | 損失函數 |
|----------|-----------|
| 回歸 | MSE（均方誤差） |
| 分類 | Cross-Entropy（交叉熵） |

---

## 避免 Overfitting 的方法

- **Dropout**：隨機丟棄部分神經元
- **L1 / L2 Regularization**：限制權重大小
- **早停（Early Stopping）**：驗證集表現變差就停止訓練
- **增加資料量**：資料多才不容易死記

---

## 延伸架構簡介

| 架構 | 應用 | 說明 |
|------|------|------|
| CNN | 圖像辨識 | 用卷積層提取局部特徵 |
| RNN / LSTM | 序列數據（語音、股價） | 能記住前後資訊 |
| Transformer / BERT | 自然語言處理 | 大幅提升語言模型效果 |

---

## 學習心得與資源

> 剛開始覺得神經網路只是很多「加加乘乘」，但其實關鍵在於：非線性激活 + 誤差反向傳播 + 正確的結構設計。

推薦資源：
- [3Blue1Brown Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)
- [DeepLizard YouTube](https://www.youtube.com/@deeplizard)

---
